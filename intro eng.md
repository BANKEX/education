# Introduction
Since you’ve decided to take a course in blockchain, we assume that you’ve already heard something about it. Let’s start with a short historic introduction and discuss the ideas that led to the emergence of blockchain.
The term ‘blockchain’ itself has a number of definitions, each of them correct to a different extent, but it is actually rather a common notion for systems similar to Bitcoin (the first successful decentralized digital currency system) than anything else. 
The creator of Bitcoin known under Satoshi Nakamoto pseudonym published the protocol description back in October 2008 and the network itself was launched in January 2009. Since then, Bitcoin drew a lot of interest to its functionality, set in motion the research of ways to apply the technology behind it to every field of human endeavour, and even became the subject of online courses.
Thanks to this excitement, it might seem that Bitcoin just appeared out of the blue, while in reality the system has combined several already existing ideas and the actual Bitcoin emergence followed 30 years of scientific work in various seemingly disjoint directions.
First among these directions was the idea to verify documents by timestamping them and binding them in chains. This idea was first introduced in papers by Stuart Haber and Scott Stornetta in 1991. They planned to create something they called a ‘digital notary’: for patents, contracts and other documentation it could be helpful to establish that they have been created before a certain point in time. Haber and Stornetta proposed to solve this problem by using a digital signature.
Digital signature is a method of proving the authorship of a message with a pair of keys — one private and one public. Private key allows to encrypt a message in such manner that the only way to decrypt its result (‘signature’) and return the source data is with a public key. This method makes it possible to ensure that the person sending the signature actually has the private key.
Haber and Stornetta proposed that the creator signs the document itself, the timestamp marking its creation, and the previous document. It results in a chain of documents, and each one of them links to the previous one. Since digital signature is designed in such a way that its validity depends on the document’s contents, whenever somebody tries to change one of the documents or the timestamp, it breaks the whole chain. Nowadays this structure is implemented in the Git system used in software development for the source code storage and versions control: all the commits there are timestamped and linked in a chain, which makes it impossible to modify a single commit or its timestamp without overwriting the whole chain.
Throughout the 1990s Haber and Stornetta wrote several more papers suggesting new approaches to developing and enhancing their idea.
Their first suggestion was to use hash instead of a digital signature. Hash functions invented back in 1950s are the functions that take as an input a message of arbitrary length and return a bit string of a fixed length called ‘hash’. We will be talking about cryptographic hash functions, which are hash functions with the following qualities:

First — hash functions always return the same hash for the same input message
Second — irreversibility, that is, impossibility to calculate the source message using hash only.
Third — resistance to the collision of the first kind, or impossibility to find a message with the same hash as the source one, and resistance to the collision of the second kind, that is, impossibility to find two messages with the same hash.
The crucial quality of hash functions is that they constitute a digest, that is, a representation of data in encrypted and compressed form; it means that even minor change of data leads to a complete change of hash. In comparison to the digital signature, the hash is faster and easier to compute, and in the case of ‘digital notary’ it confirms data integrity just as well.
Another improvement Haber and Stornetta thought of: instead of linking documents one by one, why not group the ones that are created simultaneously (or almost simultaneously) in blocks, and then bind the blocks together. If one takes a set of documents and adds to it the hash of the previous block, and then uses the hash of the resulting data, it becomes impossible to change any document in the blockchain without it being noticed, since it would change the hash of the block that contains that document, then the hash of the next block, and so on.
Finally, their third suggestion was to replace the linear document chain inside the block by the Merkle tree — another concept implemented in Bitcoin system in 10 years, that we will discuss in detail during the course.
Merkle trees (hash trees) are named after Ralph Merkle, one of the leading cryptologists of the late twentieth century, who came up with the idea in 1980. Merkle tree is a method of storing data not only in a compressed form, as hash allows, but also in a way that makes it very easy to check if the data contains certain information. Originally Merkle suggested this structure for checking electronic certificates stored in some global directory in such a way that, for example, a website could show a user the certificate and a short proof of its authenticity. Nowadays a number of protocols use such structure, for example, CONIKS (a key management system for communication services with end-to-end encryption) or Certificate Transparency (an experimental internet security and digital certificate validation protocol).
Let’s take a look at how the simplest Merkle tree — a binary Merkle tree — works:
data is divided into sections
each one of them is hashed
resulting hashes are paired
the sum of every pair is hashed 
new hashes are paired too and so on, until only one hash remains. It’s called Merkle tree root hash and it constitutes a digest of all data. So, just as with simple hashing, a change in every data section necessarily affects the root hash of the whole tree (since the hash of this section changes, which entails a change in all the hashes up the tree). At the same time, it is very easy to check if a certain piece of information was present in the source data: one simply needs to take the hash of that part and all the hashes up the tree along with their pairs, repeat these steps and compare the final result with the root hash of the tree.
To sum up, in the 1990s Haber and Stornetta developed a system where documents with the same timestamp were grouped up in a block, converted into Merkle tree form inside the block, and the blocks were linked between each other. This data structure that guaranteed authenticity of the text itself and its creation timestamp, coincides with Bitcoin data structure almost to the letter, and Nakamoto’s Bitcoin white paper contains references to Haber and Stornetta.
By the way, Haber and Stornetta’s ‘digital notary’ vision has been eventually brought to life: two companies — Surety in the mid-1990s and Guardtime (founded in 2007) provided a service of timestamp validation. Both companies also owe to Haber and Stornetta another interesting idea: buying some advertising space in the newspapers to publish roots of Merkle trees. This way it was possible to submit not only the document, but also all the hashes needed to validate its compliance with the published root, so the clients had the guarantee that the root hash — a crucial part of the document authenticity proof — was created before the date the newspaper had been published.
Another line of research that was completely unrelated to the first one, but also became the basis of Bitcoin system, is prevention of errors in synchronizing data between several network nodes, or, in other words, consensus building. The task is to reach an agreement between several network nodes on what data should be considered correct, following a certain protocol. There’s a variety of errors that may occur during the replication process, but the most common error category (and, therefore, the most difficult to prevent) is ‘Byzantine’ fault, named after the Byzantine Generals problem. 
The original version of this problem belongs to a Turing prize winner Leslie Lamport, who published ‘The Byzantine Generals Problem’ paper with two co-authors in 1982. The problem was formulated as follows: on the night before a battle some Byzantine generals, each commanding a portion of the army, try to reach an agreement on the action plan using their messengers — the main question is whether to attack or to retreat. Some generals may be treacherous, as well as messengers. Treacherous generals try to sabotage loyal generals’ attempts to decide on the common plan, while treacherous messengers may alter the loyal generals’ messages. The formulated target is to come up with a protocol of communication and decision-making that would allow all the loyal generals to agree on the same informed decision.
One can easily understand where this abstract formulation came from by replacing generals by distributed network nodes and messengers by connections between those nodes. Some nodes may transmit wrong information, sometimes in order to interfere with the network, while connections even between the functioning nodes may get lost or corrupt the signal. As you can see, the problem is formulated quite vaguely, it has a lot of versions, and researching them took more than 15 years.
The original work of Lamport and his colleagues was focused on the version which suggests that the connection is mostly stable, but the nodes may interfere with the network on purpose, so every deviation from the protocol considered an error. As a result, ‘Byzantine fault’ is most commonly understood as any fault occurring in information transmission inside a distributed network. In 1989 in his later work Lamport proposed Paxos protocol, named after the island in the Ionian sea, whose fictional parliament became a metaphor for the protocol. Paxos protocol could handle the problem for the case with an unstable connection, but faults in the nodes themselves were reduced to the fact that several nodes may turn off or send outdated messages, and ‘Byzantine’ faults as such weren’t taken into consideration at all. Only in 1999 a collaborated paper by Miguel Castro and Barbara Liskov proposed PBFT protocol — Practical Byzantine Fault Tolerance, which was designed for unreliable connection as well as ill-intentioned signal deviation. Since then, Paxos, PBFT and other, later protocols have been optimized times and again.
So how does Byzantine generals problem relate to the Bitcoin system? The thing is that, even though Satoshi Nakamoto didn’t refer to the works on the subject in Bitcoin white paper, he was in fact working on the same problem, even if it was by using another idea from a related area of computer science.
This idea emerged from the attempts to solve one of the main problems of distributed networks. Any distributed network is vulnerable to the attacks in which some participants disproportionally increase their influence in the network. For instance, many have faced the denial of service (DoS) attack, in which the malicious user saturates the network by sending too many requests, so that the regular users’ requests can’t be processed.
Another example is the Sybil attack, named after the subject of 1973 book about the treatment of a women suffering from dissociative identity disorder. This is an attack in peer-to-peer network, where all nodes are equal and none of them is trusted: every request is being multiplied for several users, so there is no one and only node that must necessarily be trusted. A malicious user can create a number of fake nodes, as though it were fake personalities, while in reality it is the same person (this is where the name comes from), in order to create the following state: victim sends a request, it gets multiplied and sent to several nodes, but it turns out that all those nodes are controlled by the malicious user, who returns false information (for instance, overwrites a Bitcoin wallet address of the victim or redirects the victim to a fishing website). This kind of pattern was used in the renowned Eclipse attacks, discovered in the Ethereum network a couple years back:  they keep the user from connecting to legitimate nodes of a peer-to-peer network, redirecting them to a compromised version of the blockchain. 
Apart from that, in a distributed network where the consensus is reached by the majority of nodes, for example, in case of a vote, a malicious user with enough fake nodes can artificially increase the weight of their vote for any task requiring consensus, up to the unlimited control over the network. 
The concept that Nakamoto used in his whitepaper, in the end, emerged from attempts to solve another case of abuse of network vulnerabilities. We’re talking about spam mailing lists.
In 1992 Cynthia Dwork and Moni Naor published a paper proposing a new way to combat spam. Let's accompany every email with a proof that before sending it the author has solved a problem, or ‘puzzle’, requiring some insignificant computing resources; later, in 1999, this proof is going to be named ‘Proof-of-work’. The puzzle had to be designed in such a way that a regular user could solve it on their personal computer in a few seconds, but for a spammer, who sends millions of emails, it would cost several weeks with the same equipment.
It was expected that this mechanism, since it requires significant computing resources from a spammer, would put an end to the profitability of spam mailing, since the whole business model was based on the fact that the messages cost almost nothing. At the same time, the puzzle had to be unique for every message as well as for every specific recipient, so that a malicious user couldn’t solve the puzzle once and then use the solution to send multiple messages.
Another important requirement for the puzzle was the asymmetry between the amount of computing resources required to solve it and to validate the solution: validation on the recipient’s end should be performed as fast and easy as possible. Dwork and Naor also suggested to provide a backdoor in the protocol for governmental structures so that they could send emails without solving any problems. They even came up with 3 puzzles complying with all these conditions and thus marked the beginning of a whole new research area.
Over time, Proof-of-work paradigm started drawing more and more attention. In particular, it was suggested to be used for protection from DoS attacks in the following way: if the number of requests to connect significantly increases, the server turns on the mode that requires solving a puzzle in order to get computing resources for processing the request. It was believed that, since puzzles require so much computing resources, it was infeasible that the malicious would be able to solve enough of them. There were also suggestions to use Proof-of-work in a similar way for limiting the frequency of login attempts.
Proof-of-work was also suggested to be used for web analytics integrity validation: it formed the basis of a method that allowed to count visits to a website without appealing to the third party, like Google Analytics, which would make it quite challenging to fake the results. The solution requires user visiting a website to perform some computations and send the results to the server when the session ends, so that it’s easy to check how much time the user spent on the website; those results are counted as visits. This solution allows to establish the lower limit of visiting time and to not count suspiciously short sessions as visits. This method leaves room for a certain amount of fake visits, but not enough to actually influence the overall traffic evaluation.
It’s interesting that Proof-of-work never ended up being used for spam protection — the purpose that Dwork and Naor originally had in mind. There are two reasons for that.
On the one hand, it became clear that the protocol is bad at stopping spam. The thing is that spammers often use botnets: they seize control over a large number of personal computers and organize them into a network. In such network the activity of any single computer is indistinguishable from the ordinary user activity, but collectively they possess enough computing power to send out enormous amounts of emails. On the other hand, Proof-of-work blocks fairly legitimate mass mailings (1000+) sent by an ordinary user instead of spam. In case where such user needs to compute PoW, mailings start to require too much time and CPU power. In the end, spam protection went another way — now it uses machine learning. 
In parallel to the academic research of Dwork and Naor, Adam Back, the creator of Hashcash system, came up with a similar idea. Adam came from the cryptopunks gang — they were ideological adversaries of centralization, striving to achieve anonymity and security of personal information space by the means of cryptography. In the beginning, cryptopunks communicated with each other using an anonymous mailing list, and everything else escalated from there. Among the gang members were Julian Assange, creator of Wikileaks and deniable encryption, Bram Cohen, author of Bittorent, Satoshi Nakamoto himself, importantly, and other prominent personas from the tech world in general and cryptography in particular.
Back’s idea was to use hash functions, or, more precisely, finding the appropriate result of a hash function as a puzzle required to process the request. As we have already mentioned, for all intents and purposes the result of a hash function can be considered random. It means that if one aims to find the hash complying with some pre-set condition, for example, the one whose binary record starts with a certain number of zeros, the only thing we can do is try and brute force this hash, and there is no way to speed up this process.
In practical terms it looks like this: a counter, which is a random number, is attributed to the message, the hash of the resulting data gets computed. This hash is tested against the pre-set condition, and if it doesn’t match, the counter changes and the process repeats. Since the hash depends on the data, it changes as the counter value does. This operation repeats until the sender gets an appropriate hash. In order to find a hash starting with n zeros, 2n attempts are required on average. For example, for hashes that are 256-bit long (there is 2256 of them), there’s – 2246 hashes starting with 10 zeros, so the probability that a random hash starts with 10 zeros equals 1/210. That means that the mean value for the number of attempts required to find such hash equals 210.
As well as Dwork and Naor, in the beginning Back thought that his mechanism was going to be used in spam protection. However, he went further and positioned hashcash as a system of remote value transmission, based on the simple condition that in order to use some objects as currency, these objects had to be limited in quantity and their value had to be agreed on by everyone. Solving computing puzzles meets both requirements. It’s important to understand that at that moment Back did not intend to embed a new mechanism into the existing reality: on the contrary, cryptopunks were striving to create their own world, which would be parallel to the traditional one and would consume it over time.
We should say that though hashcash met two key requirements for the digital currency system — resource limitation and equal value for everyone — it was missing some other important qualities.
	First, it didn’t solve the problem of double spending — the key problem of all digital currency systems. It lies in the fact that without a central node which would verify all the transactions it’s very hard to make sure that the value contained in a transaction hasn’t been spent from this wallet earlier. Secondly, hashcash currency couldn’t be transferred from one user to another.
In order to compensate for these shortcomings of the system, Back suggested to use hashcash together with ecash — an even older e-currency system. It was created by David Chaum, pioneer of the field, and relied on a cryptographic primitive that he invented, called ‘blind signature’. Blind signature is a protocol of data validation in which the validating side has no access to the data itself.
The mechanism of Blind signature is usually explained by analogy with an anonymous vote: let’s imagine that every voter has an envelope lined with carbon paper that has all their credentials written on the surface. The voter encloses a completed ballot in the envelope and an official signs it, so that his signature transfers to the ballot. Then the voter puts the signed ballot into a new unmarked envelope. The ballot is signed and the anonymity of the voter is intact.
Ecash system followed the same principle: the user generated a random coin and encrypted it with blind signature. The bank validated this coin with its signature, withdrew certain amount from the user’s account and granted the coin with the equivalent value, but the number of this specific coin remained unknown to the bank. 
When the user paid with the coin, the recipient sent it for validation to the bank, the bank confirmed its validity, checked that it’s not on the list of already spent coins (thus solving the problem of double spending) and transferred the equivalent sum to the recipient’s balance. After that the user could create a new coin with the same value and validate it through the bank again. Since the bank used to sign coins blindly, ecash could be spent anonymously, because the bank couldn’t establish whose coin it was by its number.
	In 1989 Chaum founded DigiCash company that proposed ecash system to banks. DigiCash signed a contract with one bank in USA — The Mark Twain Bank in Missouri, and Deutsсhe Bank and Credit Swiss hooked up the technology in Europe. However, in 1998 the company declared bankruptcy. Later Chaum explained it by the fact that along with the expansion of internet audience, the average level of user sophistication decreased, so the key advantage of ecash system — anonymity — wasn’t in high demand among the new users.
David Chaum’s input to the history of blockchain wasn’t confined to this individual failure: he’s the author of another idea which, years later, proved to be very important for Bitcoin implementation. In his work ‘Untraceable electronic mail, return addresses, and digital pseudonyms’ (1981) Chaum suggested to use the public key (as we know, it’s a part of a digital signature) as a digital pseudonym. 
The public key does indeed allow to use the private key for confirming authorship without publishing any of the standard identifiers. It’s important to point out that such approach doesn’t lead to anonymity, since all the actions of a user are performed under the same public key. This property is called ‘pseudo anonymity’: the publicly available information allows to establish the actions performed by the same author but not his personal data.
	There is an obvious problem: if the sender only knows the pseudonym of the recipient, there’s no telling to which PC the message should be send, so in this kind of system all messages have to be sent to all users. Alas, this solution is very ineffective compared to centralized systems, and even Chaum himself in his later works on digital currency turned away from this idea.
	By the way, cryptopunks were actually very interested in the possibility of communication under pseudonyms, or, as they called them, ‘nyms’. However, for them this notion meant digital addresses or nicknames that a human could remember, and public keys (long and random sets of numbers and letters) were just appended to them.
The mechanism of using public keys as pseudonyms formed the basis of another distributed digital currency system, B-money. This system was proposed by a developer and cryptographer Wei Dai in the same old cryptopunks’ mailing list, where one year before that Adam Back posted his hashcash idea. As a matter of fact, B-money system pursued Back’s idea of using PoW as a money equivalent.
	Wei suggested two protocols at once. That said, the first protocol, according to his own words, could not be implemented, since it would require communication channels, seamless and synchronous at the same time, between the nodes of the network. In this protocol, all transactions were broadcasted to all the network members, and each of them kept record of every balance. Meanwhile, transaction validity was being confirmed by digital signature. There was not supposed to be any other, more complex consensus-building. New monetary units could be also created quite easily: it was enough to broadcast to all the network members the solution of some hashing problem.
It’s interesting that this protocol provided the possibility to make a contract with the participation of an arbitrator. Both parties and the arbitrator would specify the maximum amount of monetary units as a deposit and transfer them to the special account of the contract itself. In case of a successful completion of the contract both parties broadcasted the confirmation to the network, while the funds were transferred to the parties in the default amount. If a dispute arose, it was suggested to solve it with the arbitrator’s help, and if the dispute couldn’t be settled, every party had to broadcast their solution and the arguments to back it up. It’s important to point out that this idea is very similar to the smart-contract concept we are going to discuss later.
Wei’s second protocol was positioned as a more practice-oriented one. In this protocol the whole state of the system was kept only by some of the network members, on the servers. Every time a random set of servers was used to confirm the transaction.
In order to ensure trust towards the servers, the following system was provided: every server had to send a certain share of its funds to the special account, which could be fined. Apart from that, from time to time servers had to post their version of the system state, so that every member could check if their balance was correct and compare summarized balances of all the accounts to the total amount of generated funds: it allowed to make sure that the server didn’t add any funds to any account without spending computing resources.
Wei had yet another interesting idea. He noticed that, because of the great speed of the technical progress, different network members may not have the same notions about the real value of certain computations. This is why he suggested to conduct emission in the form of an auction: members were to broadcast their suggestions on the additional amount of funds to be released, and then place the bids equal to the amount of computations they were ready to perform in order to get the funds on their account.
Wei’s system has never been implemented, but it’s useful to know about it, since it was the first system that reminded of Bitcoin in terms of structure. This is probably why the smallest fractional unit of ether — the internal currency of the most popular blockchain system Ethereum — is called ‘wei’. 
BitGold system came even closer to Bitcoin. It was introduced in 2005 by Nick Sabo, after whom another fractional unit of ether is named. According to one of the most popular theories, it was Nick Szabo who hid behind the Satoshi Nakamoto pseudonym. 
Szabo proposed to use a hash function based PoW with a timestamp as a currency. Moreover, currency units themselves would link into the chain where final bits of the previous PoW would become the puzzle that needed to be solved in order to find the next one. 
At the same time, the information about the correlation between public keys and monetary units they possessed was stored in a separate distributed ownership database.  Szabo described such database as early as in 1998 and suggested to use it for decentralized accounting of any ownership rights. It’s interesting that in this system in order to transfer ownership rights, the sender would use his private key to sign the corresponding certificate of property rights and the public key of the recipient. Thus, a property record contained information about the whole chain of ownership; this structure reminds the ideas of Haber and Stornetta a lot, and later it was implemented in Bitcoin almost to a “T”.
That means that, much like in the digital currency systems proposed earlier, full transaction history was stored on all the network nodes. The truth of the information was being reached by consensus within the protocol, the general idea of which came down to dividing sets of nodes into subsets so that any two of them would intersect, and the intersection would contain ⅔ honest nodes. Then any of these subsets could represent the whole system while Byzantine fault resistance could be reached provided that there were no more than ¼ malicious nodes. That being said, the key problem of BitGold was vulnerability to the Sybil attack, because consensus was being reached by the majority of nodes, and the number of nodes could be increased artificially.
Naturally, Szabo was a former cryptopunk as well, and actually the thing that interested him above all else was applying cryptography and computer science achievements to move legal framework into the digital space. While working on this problem, he invented smart contracts — a further development of the protected distributed database concept; the specifics of smart contracts is that instead of distributed data they use distributed computations. This is an invention that Szabo is now best known for. 
Smart contracts are programs in distributed network that can be addressed to by any network member. It’s possible not only to store data in them, but also to call their functions which will be carried out by the network nodes strictly in accordance with the contract’s internal logic, as well as with the network rules. And yet, the results of these computations can be trusted without repeated computations, because they are being verified by many network nodes. Smart contracts make for an especially powerful tool if they are implemented on top of the digital currency system, because in this case they can run it on their own.
While the concept of a protected distributed database was originally discussed in the context of digital currency, smart contracts were proposed by Szabo as a digital equivalent of legal contracts, where the control over contract completion is performed algorithmically. This approach was received with criticism, because an algorithmic control system has a lot of limitations, which means that smart contracts can’t serve as a full-fledged replacement for legal documents.
In some sense, Bitcoin transactions can be regarded as smart contracts, because they allow to implement certain logic, like collaborative confirmation or delay of the transfer, but only within strictly determined limitations. The possibility to create smart contracts that support any computational logic, that is, write them in a Turing-complete language, emerged only in 2014 along with the previously mentioned blockchain network Ethereum. 
However, before we can figure out in detail how Ethereum and transactions in Bitcoin work, and even how all these scattered ideas merged into the Bitcoin network, we have to fully understand the 3 main technologies underlying everything that is called blockchain: namely, cryptographic hashing, digital signature and p2p-networks .
Stay tuned!
