# Introduction
Мы предполагаем, что раз вы решили пройти курс про блокчейн, то вы о нем что-то уже слышали. Мы начнем с краткой исторической вводной и обсудим, какие идеи предшествовали появлению блокчейна.
У самого термина “блокчейн” есть много определений разной степени корректности, но на самом деле это скорее общее понятие для обозначения систем, схожих с Bitcoin – первой успешной децентрализованной системой электронной валюты.
Автор Bitcoin, известный под псевдонимом Сатоши Накамото, опубликовал описание протокола в октябре 2008 года, а сама сеть была запущена в январе 2009. С тех пор к возможностям Bitcoin возник невероятный интерес, а техническим решениям, лежащим в основе его реализации, стали искать применение практически во всех областях человеческой деятельности и даже посвящать онлайн-курсы.
Такой ажиотаж создал у многих ощущение, что Bitcoin возник внезапно и как бы на ровном месте. Но в действительности Bitcoin объединил в себе сразу несколько уже существовавших идей, и его возникновению предшествовали без малого 30 лет научных работ в разных, на первый взгляд непересекающихся направлениях.
Первым таким направлением была идея верифицировать документы, проставляя на них время создания и увязывая в цепочки.  Эта идея была впервые озвучена в серии статей исследователей Стюарта Хэйбера и Скотта Сторнетты в 1991 году. Они хотели создать что-то наподобие “цифрового нотариуса”: для патентов, договоров и других документов может быть полезным установить, что они были созданы не позднее некоторого момента во времени. Хэйбер и Сторнетта предложили решать эту задачу с помощью электронной подписи.
Цифровая подпись – механизм подтверждения авторства сообщения при помощи пары ключей, приватного и публичного. Приватный ключ позволяет зашифровать сообщение так, что результат, называющийся подпись,можно расшифровать публичным ключом и получить исходные данные. Таким образом можно убедиться, что посылающий подпись действительно владеет приватным ключом. 
Идея Хэйбера и Сторнетты заключалась в том, что создатель подписывает сам документ, время его подписания и предыдущий документ. В результате возникает цепочка документов, каждый из которых содержит в себе отсылку к предыдущему. Поскольку электронная подпись устроена так, что ее валидность зависит от содержания документа, если кто-то попытается изменить один из документов или пометку о времени его создания, это нарушит всю цепочку. По такой схеме сегодня работает известная система Git, которую используют при разработке программного обеспечения для хранения исходных кодов и контроля версий: в ней все коммиты связаны в цепочку, и на каждом проставлено время создания, так что невозможно изменить содержимое отдельно взятого коммита или его временную метку, не переписав при этом всю цепочку.
На протяжение 90-ых Хэйбер и Сторнетта выпустили еще несколько статей, в которых предложили ряд способов развить и усовершенствовать эту идею.
Первым их предложением было заменить цифровую подпись на хеширование. Хеш-функции, изобретенные еще в 50-ых, – это функции, которые принимают на вход сообщение любой длины, а на выходе выдают битовую строку фиксированной длины, которая называется хеш. Мы будем говорить о криптографических хеш-функциях, то есть таких хеш-функциях, которые обладают рядом свойств. 
Первое – для одного и того же сообщения они всегда выдают один и тот же хеш.
Второе – необратимость, то есть невозможность вычислить исходное сообщение, имея только хеш.
Третье – устойчивость к коллизиям первого рода, то есть невозможность найти сообщение с тем же хешом, что и исходное, а также коллизиям второго рода, то есть невозможность найти два сообщения с одним и тем же хешом.
Главное свойство хешей – то, что они представляют собой digest, то есть, репрезентацию данных в зашифрованном и сжатом виде; это значит, что стоит хоть немного поменять данные, и их хеш изменится полностью. По сравнению с цифровой подписью хеширование вычисляется быстрее и проще, но для случая с цифровым нотариусом ничуть не хуже подтверждает сохранность данных. 
Второе усовершенствование: вместо того, чтобы связывать документы по одному, те из них, что созданы одновременно (или почти одновременно), можно объединять в группы, или блоки, а уже блоки связывать между собой. Если к набору документов присоединить хеш предыдущего блока, а затем взять хеш от получившихся данных, то невозможно будет незаметно изменить ни один документ в цепочке блоков, поскольку это изменит хеш его блока, а значит, и хеш следующего блока и далее по цепочке.
И, наконец, третье их предложение: внутри блока можно заменить линейную цепочку документов на дерево Меркла  – еще один концепт, который мы будем подробно обсуждать в курсе и который спустя 10 лет нашел применение в системе Bitcoin.
Деревья Меркла (хэш-дерево, дерево хэшей, Merkle tree(s)) названы по имени одного из крупнейших криптологов конца 20 века, Ральфа Меркла, который и предложил идею в 1980 году. Дерево Меркла – это способ хранить данные не просто в сжатом виде, как это позволяет делать хеширование, но в таком, чтобы было максимально просто проверить, содержится ли среди этих данных та или иная информация. Изначально Меркл предлагал эту структуру для проверки электронных сертификатов, хранящихся в некой глобальной директории, с тем, чтобы, например, сайт предоставлял посетителю сертификат и короткое доказательство его истинности. Сейчас такая структура широко применяется в целом наборе протоколов.
Посмотрим, как устроен самый простой тип дерева Меркла – бинарный: 
данные разделяются на части
каждая из них хешируется
полученные хеши объединяются в пары
берется хеш от суммы каждой пары
новые хеши тоже объединяются в пары, и так далее, пока не останется один хеш. Он называется корневым хешем дерева Меркла и представляет собой digest всех данных. То есть, как и для простого хеширования, изменение в любой их части непременно отразится в корневом хеше всего дерева (поскольку изменится хеш самой этой части, а значит – и все хеши вверх по дереву). Но и проверить, входила ли та или иная часть информации в исходные данные, тоже очень легко: нужно взять хеш этой части и все хеши вверх по дереву вместе с их парами, повторить описанные действия и сравнить результат с корневым хешем дерева.
Итак, Хэйбер и Сторнетта уже в 90-х годах разработали систему, где документы с общим временем создания объединялись в блок, внутри блока приводились к виду дерева Меркла, а блоки связывались между собой. Эта структура данных, гарантирующая подлинность текста и метки о времени публикации документа, почти буквально совпадает со структурой данных Bitcoin, и Накамото ссылался на работы Хэйбера и Сторнетты в Bitcoin white paper.
Кстати, замысел Хэйбера и Сторнетты про “цифрового нотариуса” был-таки воплощен в жизнь: две компании – Surety в середине 90-ых и Guardtime, основанная в 2007 – оказывали услуги по подтверждению времени создания документов. Обе компании почерпнули из работ этих авторов и еще одну любопытную идею: публиковать корни деревьев Меркла в газетах, выкупая для этого небольшую рекламную площадь. Так вместе с документом можно было предоставить все хеши, требующиеся для проверки его соответствия опубликованному корню, и клиенты получали гарантию, что корневой хеш – ключевой элемент доказательства подлинности документа – возник не позже даты публикации газеты.
Другое направление исследований, которое никак не было связано с первым, но вместе с ним легло в основу системы Bitcoin – это предотвращение ошибок при синхронизации данных между несколькими узлами сети, или, другими словами, при достижении консенсуса. Задача состоит в том, чтобы несколько узлов сети, следуя определенному протоколу, договорились, какие данные считать верными. В процессе репликации может возникнуть множество разных ошибок, но самой общей категорией ошибки, а значит, и наиболее сложной для предотвращения является “византийская” ошибка, названная в честь задачи о византийских генералах.
Изначальная формулировка этой задачи принадлежит лауреату премии Тьюринга Лесли Лампорту, который вместе с двумя соавторами опубликовал одноименную статью “Задача о византийских генералах” в 1982 году. Задача, собственно, такая: некоторое количество византийских генералов со своими армиями в ночь перед сражением пытаются при помощи посыльных договориться об общем плане действий – атаковать им или отступать, при этом некоторые генералы и посыльные могут быть предателями. Генералы-предатели целенаправленно пытаются помешать верным генералам принять общий план действий, а посыльные-предатели могут искажать сообщения верных генералов. Вопрос в задаче – придумать такой протокол обмена сообщениями и принятия решений, чтобы все верные генералы приняли одно и то же обоснованное решение.
Откуда взялась эта абстрактная формулировка, становится яснее, если заменить генералов на узлы распределенной сети, а посыльных – на соединение между ними. Некоторые узлы могут передавать неверную информацию, иногда преследуя цель помешать работе сети, а соединения, даже между исправными узлами, могут пропадать или искажать сигнал. Как вы могли заметить, задача сформулирована достаточно размыто, у нее может быть множество версий, и на их исследование ушло больше 15 лет. 
В оригинальной работе Лампорта и его коллег фокус был на том варианте задачи, где связь по большей части надежна, но узлы могут злонамеренно нарушать работу сети, то есть за ошибку считается любое отклонение от протокола. В результате за словосочетанием “византийская ошибка” закрепилось максимально общее понимание любой ошибки при передаче информации в распределенной сети. В более поздней работе в 1989 году Лампорт предложил протокол Paxos, названный в честь острова в Ионическом море, вымышленный парламент которого послужил метафорой для протокола. Paxos решал задачу при условии ненадежности связи, но ошибки самих узлов сводились к тому, что небольшое их количество может отключаться или отправлять устаревшие сообщения, а собственно “византийских” ошибок протокол не подразумевал вообще. И только в 1999 году совместная работа Мигеля Кастро и Барбары Лисков предложила протокол PBFT – Practical Byzantine Fault Tolerance, который одновременно рассчитан и на ненадежность связи, и на злонамеренное искажение сигнала. С тех пор и Paxos, и PBFT, и другие, возникшие позже протоколы были многократно оптимизированы. 
Какое же отношение задача о византийских генералах имеет к системе Bitcoin? Дело в том, что Сатоши Накамото, хоть и не ссылался в Bitcoin white paper на работы из этой области, тем не менее, решал именно эту задачу, правда, используя еще одну идею из смежной ветви компьютерной науки.
Эта идея возникла из попыток решить одну из основных проблем распределенных сетей. Любая распределенная сеть подвержена атакам, при которых некоторые участники непропорционально увеличивают свое влияние в ней. Например, многие сталкивались с атакой denial of service (dos), когда злоумышленник перегружает сеть, посылая в нее слишком много запросов, так что запросы обычных пользователей перестают обрабатываться. 
Другой пример – атака Сибиллы, названная в честь героини книги 1973 года о лечении женщины с диссоциативным расстройством личности. Эта атака в одноранговой сети, то есть сети, где все узлы равны и ни один из них не является доверенным: каждый запрос дублируется нескольким получателям, чтобы не оказалось единственного узла, ответу которого было бы необходимо полностью доверять. Злоумышленник может создать большое количество фиктивных узлов, как бы ненастоящих личностей, на самом деле являющихся одним и тем же человеком – отсюда название, – с целью создать следующую ситуацию: жертва отправляет запрос, он дублируется в несколько узлов, но оказывается, что все эти узлы контролирует злоумышленник, который в результате возвращает жертве неверную информацию. На такой схеме основаны известные атаки типа Eclipse, обнаруженные в сети Ethereum несколько лет назад назад: они не позволяют пользователю подключиться к легальным узлам сети peer-to-peer, перенаправляя их в скомпрометированную версию блокчейна. Кроме того, если в распределенной сети консенсус достигается большинством узлов, например,при помощи голосования, злоумышленник, имея много фиктивных узлов, может искусственно увеличить свой вес при принятии любого решения, требующего консенсуса, вплоть до полного контроля над работой сети.
Однако концепт, который в итоге использовал Накамото, возник из попыток решить еще один кейс злонамеренного использования уязвимости сети. Речь идет об организации спам-рассылки.
В 1992 году Синтия Дворк и Мони Наор опубликовали работу, в которой предложили новый способ борьбы со спамом. Пусть к каждому e-mail прилагается доказательство того, что перед отправкой его автор решил задачу, или “пазл”, который требует некоторого незначительного использования вычислительных ресурсов; позже, уже в 1999, это доказательство получит название – Proof-of-work. Пазл должен был быть таким, чтобы обычный пользователь мог решить ее на своем персональном компьютере за несколько секунд, но у спамера, отсылающего миллион писем, на том же оборудовании это занимало бы несколько недель. Предполагалось, что из-за этого механизма, требующего от спамера значительной затраты ресурсов, спам-рассылка перестанет быть выгодным предприятием, поскольку вся эта бизнес-модель строится на том, что сообщения практически ничего не стоят. При этом пазл должен быть уникальным как для каждого сообщения, так и для каждого конкретного получателя, чтобы злоумышленник, найдя решение для одного пазла, не мог использовать его для отправки множественных сообщений.
Еще одно важное требование к пазлу – асимметрия вычислительных ресурсов, требуемых на его решение и на проверку: проверка на стороне получателя должна выполняться максимально быстро и просто. Кроме того, Дворк и Наор предлагали предусмотреть в протоколе бэкдор для государственных организаций, чтобы те могли отправлять письма, не решая задач, и даже придумали 3 пазла, удовлетворяющих всем этим условиям, чем положили начало целой области исследований.
Со временем парадигма Proof-of-work привлекала к себе все больше внимания. В частности, ее предлагали использовать для защиты от DOS-атак по следующей схеме: при сильном увеличении количества запросов на соединение сервер включает режим, при котором ресурс на обработку запроса выделяется только после того, как клиент решит пазл. Считалось, что, поскольку пазлы требуют много вычислительных ресурсов, вероятность того, что злоумышленник сможет решить их в нужном количестве, пренебрежимо мала. Примерно по такому же принципу Proof-of-work предлагали использовать для ограничения частоты попыток ввода пароля.
	Еще схему Proof-of-work предлагалось использовать для подтверждения целостности веб-аналитики: на ее основе придумали способ подсчитывать посещения сайтов, не апеллируя к третьей стороне, вроде Google Analytics, при котором достаточно сложно подделать результат. Решение такое: пользователь, находясь на сайте, постепенно выполняет некоторые вычисления, а при завершении сессии отправляет на сервер результат, по которому легко проверить, сколько именно времени он провел на сайте; эти результаты и засчитываются как посещения. Такое решение позволяет установить нижнюю границу по времени посещения, чтобы не включать в статистику неправдоподобно короткие сессии. При этом все равно есть возможность накрутить какое-то количество фейковых просмотров, но не такое, чтобы оно могло сказаться на общей оценке посещаемости.
Любопытно, что в целях, которые Дворк и Наор имели в виду изначально – защита от спама – механизм Proof-of-work в итоге применять не стали. На то есть 2 причины.
С одной стороны, как оказалось, он плохо останавливает спам. Дело в том, что спамеры очень часто используют бот-неты: они захватывают контроль над большим количеством персональных компьютеров и организуют их в сеть. В такой сети активность каждого отдельно взятого компьютера неотличима от активности обычного пользователя, а в совокупности они обладают достаточной вычислительной мощностью, чтобы рассылать огромные массивы писем. С другой стороны, вместо спама Proof-of-work блокирует вполне легитимные массовые рассылки, когда обычный пользователь отправляет 1000+ писем. Если такому пользователю необходимо вычислять PoW, рассылки начинают требовать очень больших затрат времени и процессорной мощности. Так что защита от спама в итоге пошла по другому пути: сейчас для этого используют machine learning.
Параллельно с академическим исследованием Дворк и Наора к похожей идее пришел Адам Бэк, создатель системы Hashcash. Адам вышел из тусовки шифропанков – идейных противников централизации, стремившихся достичь анонимности и безопасности личного информационного пространства при помощи криптографии. Шифропанки первоначально общались между собой посредством анонимной email-рассылки. В эту тусовку входили Джулиан Ассанж, создатель Викиликс и отрицаемой криптографии, автор Bittorrent Брэм Коэн, сам Сатоши Накамото (что немаловажно), и многие другие известные фигуры в мире технологий вообще и криптографии в частности. 
Идея Бэка заключалась в том, чтобы в качестве пазла, который требуется решить для обработки запроса, использовать хеш-функции, точнее, подбор подходящего их результата. Как мы уже упоминали, для любых практических целей результат хеш-функции можно считать случайным. Это значит, что если у нас есть цель найти хеш, удовлетворяющий какому-то заранее установленному условию, например, двоичная запись которого начинается с определенного количества нулей, единственное, что мы можем сделать – это пытаться этот хеш подобрать, и никак ускорить этот процесс не выйдет.
На практике это выглядит так: к сообщению приписывается счетчик, то есть случайное число, и вычисляется хеш получившихся данных. Он проверяется на соответствие установленному условию, и если не подходит, счетчик изменяется, и операция повторяется. Поскольку хеш зависит от данных, при изменении счетчика изменится и хеш. Это происходит до тех пор, пока не получится хеш нужного вида. Чтобы найти хеш, начинающийся с n нулей, в среднем придется совершить 2n попыток. Например, среди хешей длиной 256 бит, которых всего 2256, хешей, начинающихся с 10 нулей, – 2246,  значит, вероятность того, что случайный хеш будет начинаться с 10 нулей – 1/210. То есть, матожидание количества попыток в переборе для нахождения такого хеша – 210.
Как и Дворк и Наор, Бэк изначально рассчитывал, что его механизм будет использоваться для защиты от спама. Однако он пошел дальше и позиционировал hashcash как систему удаленной передачи ценности, опираясь на простое условие, что, для того чтобы использовать некоторые объекты как деньги, нужно, чтобы эти объекты были доступны в ограниченных количествах, а их ценность всеми воспринималась одинаково. Решение вычислительных пазлов соответствует этим параметрам. Здесь важно понимать, что в тот момент Бэк не ставил перед собой задачу встроить новый механизм в существующую действительность: напротив, шифропанки стремились создать свой мир, который был бы параллелен традиционному, а со временем поглотил бы его.
Нужно сказать, что хотя hashcash и отвечал двум ключевым требованиям для системы электронной наличности – ограниченность ресурса и его одинаковая для всех ценность – некоторых других важных свойств ему не хватало.
Во-первых, в нем не была решена проблема double spending – ключевая проблема всех систем электронной наличности. Она состоит в том, что при отсутствии центрального узла, верифицирующего все транзакции, очень трудно проверить, что средства, содержащиеся в транзакции, не были уже потрачены с этого кошелька ранее. Во-вторых, валюту hashcash невозможно было передавать от одного юзера к другому.
Чтобы компенсировать эти два недостатка системы, Бэк предлагал использовать hashcash совместно с ecash – еще более ранней системой в области электронной наличности. Она была изобретена Дэвидом Чаумом, пионером этой области, и опиралась на изобретенный им криптографический примитив под названием “слепая подпись”. Слепая подпись – это такой протокол валидации данных, при котором валидирующая сторона не имеет доступа к самим данным. 
Механизм слепой подписи обычно объясняют через аналогию с анонимными выборами: представим себе, что у каждого голосующего есть конверт, на котором снаружи написаны его данные, а изнутри он выложен копиркой. Голосующий кладет в конверт заполненный бюллетень, а представитель власти этот конверт подписывает – и его подпись отпечатывается на бюллетене. Затем голосующий перекладывает уже заверенный бюллетень в чистый конверт. Бюллетень заверен, анонимность голосующего сохранена.
Система ecash работала по тому же принципу: юзер генерировал случайную монету и шифровал ее слепой подписью. Банк валидировал монету своей подписью, снимая со счета юзера установленную сумму и наделяя монету эквивалентной ценностью, при этом не зная номера этой конкретной монеты.
Когда юзер расплачивался монетой, получатель отправлял ее на проверку в банк, который подтверждал ее валидность, проверял, нет ли ее номера в списке уже потраченных монет (решая проблему double spending), и зачислял эквивалентную сумму на баланс получателя. Далее получатель мог создать новую монету на эту же сумму и тоже валидировать ее через банк. Поскольку банк подписывал монеты вслепую, ecash можно было тратить анонимно: определить по номеру монеты, чья она, банк не мог.
В 1989 году Чаум основал компанию DigiCash, которая предлагала банкам воспользоваться системой ecash. Компания подписала контракт с одним банком в США, – The Mark Twain Bank в Миссури, в Европе технологию подключили Deutsche Bank и Credit Swiss. Однако уже в 1998 году компания объявила о банкротстве. Чаум позднее объяснял это тем, что с расширением аудитории интернета снижалась средняя искушенность пользователя, и ключевое преимущество системы ecash – анонимность – новых пользователей уже особенно не интересовало.
Вклад Дэвида Чаума в историю блокчейна не ограничился этой локальной неудачей: ему принадлежит еще одна идея, которая впоследствии оказалась очень важна для реализации bitcoin. В своей работе “Неотслеживаемая электронная почта, обратные адреса и электронные псевдонимы” 1981 года Чаум предложил использовать публичный ключ (как мы помним, это элемент цифровой подписи) в качестве цифрового псевдонима.
Так вот, публичный ключ, действительно, позволяет использовать приватный ключ для подтверждения авторства, не публикуя стандартные идентификаторы.
Важно отметить, что такой подход не достигает анонимности, так как все действия пользователя совершаются от имени одного и того же публичного ключа. Такое свойство называется псевдонимностью: публично доступная информация позволяет установить действия, принадлежащие одному и тому же автору, но не его личные данные.
Здесь есть очевидная проблема: если известен только псевдоним, неизвестно, на какой компьютер отправлять сообщение, поэтому все сообщения в такой системе приходится отправлять всем ее участникам. Такое решение, увы, крайне неэффективно в сравнении с централизованными системами, и даже сам Чаум в более поздних работах, посвященных электронной наличности, стал отходить от этой идеи.
Шифропанков, между прочим, очень интересовала возможность коммуникации под псевдонимами или, как они их называли, “нимами”. Однако они под этим понятием подразумевали электронные адреса или ники, которые мог бы запомнить человек, а публичные ключи (длинные и случайные наборы цифр и букв) только присоединялись к ним.
Механизм публичных ключей как псевдонимов лег в основу другой распределенной системы электронной наличности, B-money. Эту систему предложил разработчик и криптограф Вэй Дай в той же рассылке шифропанков, где Адам Бэк годом ранее опубликовал идею hashcash. Собственно, система B-money продолжала идею Бэка использовать Proof-of-work в качестве денежного эквивалента.
Wei предложил сразу два протокола. Правда, первый, по его собственным словам, был неприменим на практике, так как требовал, чтобы между узлами сети существовали каналы связи, которые были бы одновременно бесперебойными и синхронными. В этом протоколе все транзакции транслировались всем участникам сети, каждый из которых фиксировал все балансы. При этом валидность транзакции подтверждалась электронной подписью. Никакого более сложного механизма консенсуса не предполагалось. Новые денежные единицы тоже создавались достаточно простым образом:  достаточно было транслировать всем участникам сети решение некой задачи хеширования.
Любопытно, что в рамках этого протокола подразумевалась возможность заключить контракт с участием арбитра. Обе стороны и арбитр указывали максимальный объем денежных единиц в качестве обеспечения и переводили их на специальный счет самого контракта. В случае успешного исполнения контракта, стороны транслировали в сеть подтверждение, а деньги автоматически переводились сторонам в установленном по умолчанию количестве. Если возникал спор, его предлагалось решать при помощи арбитра, а в случае невозможности разрешения спора, каждый участник должен был транслировать свой вариант разрешения, а также аргументы в его пользу. Важно заметить, что эта идея очень схожа с концепцией смарт-контрактов, о которой мы будем говорить чуть позже.
Второй протокол Вэя позиционировался как более ориентированный на практику. В нем полное состояние системы хранилось только у части участников сети – на серверах. При этом для подтверждения транзакции каждый раз использовался случайный набор серверов. 
Чтобы обеспечить доверие к серверам, предусмотрена была такая система: каждый сервер должен был отправить некоторую часть денег на специальный аккаунт, из которого могли взиматься штрафы. Кроме того, время от времени сервера должны были публиковать свою версию состояния системы, чтобы каждый участник мог проверить корректность своего баланса, а также сравнить общую сумму балансов по всем счетам с общим количеством сгенерированных денег – так можно было гарантировать, что сервер не записал на чей-то баланс деньги без затрат вычислительной мощности.
 У Вэя была и еще одна интересная идея. Он заметил, что из-за большой скорости технического прогресса у разных участников системы могут не совпадать представления о реальной стоимости тех или иных вычислений. Поэтому он предложил проводить эмиссию в виде аукциона: участники должны были транслировать свои предложения о дополнительном количестве денег для выпуска, а потом делать ставки в объеме вычислений, которые они готовы выполнить, чтобы получить эти деньги на свой счет.
Система Вэя не была имплементирована, но о ней полезно знать, потому что это была первая система, структурно похожая на bitcoin. Вероятно, именно поэтому самая маленькая дробная часть Эфира – внутренней валюты в самой популярной блокчейн-системе Ethereum – называется wei.
Еще ближе к биткоину подошла система BitGold. Ее предложил в 2005 году Ник Сабо, в честь которого тоже названа дробная часть Эфира. Согласно одной из популярных теорий, именно Ник Сабо и скрывался за псевдонимом Сатоши Накамото.
Сабо предлагал использовать в качестве валюты основанный на хеш-функции Proof-of-Work с временной отметкой. Причем сами единицы валюты увязывались в цепочку, где последние биты предыдущего POW становились задачей для нахождения следующего.
При этом информация о соответствии между публичными ключами и теми денежными единицами, которыми они владеют, хранилась в отдельной распределенной базе данных собственности. Описание такой базы данных Сабо дал еще в 1998 году, предложив использовать ее для децентрализованного учета любых прав собственности. Интересно, что в этой системе для передачи прав собственности отправитель подписывал своим приватным ключом соответствующий акт о правах на имущество и публичный ключ адресата. Таким образом, в записи о собственности хранилась информация о всей цепочке владельцев – структура, очень напоминающая идею Хейбера и Сторнетты, и позже почти в точности реализованная в Bitcoin.
Это значит, что, как и в предложенных ранее системах электронной валюты, полная история хранилась на всех узлах сети. Верность информации достигалась консенсусом при помощи протокола, общая идея которого сводилась к разбиению множества узлов на подмножества, так чтобы любые два пересекались, и в пересечении содержалось ⅔ честных узлов. Тогда любое из этих подмножеств могло выступать от лица всей системы, а устойчивость к византийской ошибке достигалась при условии, что зловредных узлов менее ¼ от всех. При этом ключевой проблемой BitGold была уязвимость к атаке Сибиллы, поскольку консенсус определялся большинством узлов, количество которых можно было искусственно увеличить.
Сабо, разумеется, тоже бывший шифропанк, и вообще-то его в первую очередь занимала задача применить достижения криптографии и компьютерной науки, чтобы перевести юридическое поле в цифровое пространство. Работая над решением этой задачи, он изобрел смарт-контракты – дальнейшее развитие идеи защищенной распределенной базы данных; их отличие в том, что вместо данных распределенными являются вычисления. И именно этим изобретением Сабо наиболее известен.
Смарт-контракты – это программы, существующие в распределенной сети, к которым может обращаться любой ее участник. Можно не только хранить в них данные, но и вызывать их функции, которые будут выполняться узлами сети строго в соответствии с внутренней логикой контрактов, а также с правилами сети, при этом результату этих вычислений можно доверять, не повторяя вычислений, поскольку они верифицируются многими узлами сети. Смарт-контракты являются особенно сильным инструментом, если они реализованы поверх системы электронной валюты, поскольку в этом случае они могут ею самостоятельно управлять.
Если идея защищенной распределенной базы данных изначально обсуждалась в контексте электронной валюты, то смарт-контракты Сабо предлагал в качестве электронного аналога юридических контрактов, где контроль исполнения осуществляется алгоритмически.
В некотором смысле, транзакции в Bitcoin можно считать смарт-контрактами, поскольку они позволяют реализовывать некоторую логику, вроде совместного подтверждения или задержки перевода, но только в строго определенных пределах. Возможность создавать смарт-контракты, поддерживающие любую вычислительную логику, то есть написанные на Тьюринг-полном языке, появилась только в 2014 году, вместе с уже упоминавшейся блокчейн-сетью Ethereum.
Но прежде, чем мы сможем во всех подробностях разобраться с тем, как работает Ethereum, как устроены транзакции в Bitcoin, и даже с тем, как все эти разрозненные идеи объединились в саму сеть Bitcoin, мы должны как следует понять, как работают 3 основные технологии, лежащие в основе всего, что называется блокчейн – криптографическое хеширование, электронная подпись и p2p-сети.
Stay tuned!
